-- phpMyAdmin SQL Dump
-- version 5.0.2
-- https://www.phpmyadmin.net/
--
-- Host: 127.0.0.1
-- Generation Time: Aug 02, 2020 at 11:59 AM
-- Server version: 10.4.11-MariaDB
-- PHP Version: 7.2.31

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `datarevolution`
--

-- --------------------------------------------------------

--
-- Table structure for table `contacts`
--

CREATE TABLE `contacts` (
  `sno` int(50) NOT NULL,
  `name` text NOT NULL,
  `email` varchar(50) NOT NULL,
  `phone_num` varchar(50) NOT NULL,
  `msg` text NOT NULL,
  `date` datetime DEFAULT current_timestamp()
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

--
-- Dumping data for table `contacts`
--

INSERT INTO `contacts` (`sno`, `name`, `email`, `phone_num`, `msg`, `date`) VALUES
(1, 'first post', 'firstpost@gmail.com', '123', 'first post', '2020-07-19 16:52:21'),
(4, 'swapnil pawar', 'swapnil.pawar4470@gmail.com', '8888888888', 'you are genious', '2020-07-19 16:53:49'),
(5, 'swapnil pawar', 'swapnil.pawar4470@gmail.com', '8806348888', 'you are genious', '2020-07-19 16:54:13'),
(6, 'yuvo', 'yuvi.pawar4770@gmail.com', '8899776655', 'thank you', '2020-07-19 16:55:17'),
(7, 'yuvraj', 'swapnil.pawar68@gmail,com', '8793166645', 'hi', '2020-07-20 18:28:06');

-- --------------------------------------------------------

--
-- Table structure for table `posts`
--

CREATE TABLE `posts` (
  `sno` int(11) NOT NULL,
  `title` text NOT NULL,
  `tagline` text NOT NULL,
  `slug` varchar(25) NOT NULL,
  `content` text NOT NULL,
  `img_file` varchar(12) NOT NULL,
  `date` datetime NOT NULL DEFAULT current_timestamp()
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

--
-- Dumping data for table `posts`
--

INSERT INTO `posts` (`sno`, `title`, `tagline`, `slug`, `content`, `img_file`, `date`) VALUES
(1, 'Best Data Visualization Tools Data Scientists Need To Use', 'Data Visualization Tools', 'first-post', 'In this article, we will be discussing some of the best data visualization tools that data scientists need to try, in order to make the process smooth while achieving valuable results.\r\nWhat is Data Visualization? \r\nData Visualization is basically putting the analyzed data in the form of visuals i.e - graphs, images. These visualizations make it easy for humans to understand the analyzed trends through visuals.\r\n\r\nData Visualization is very important when it comes to analyzing big datasets. When data scientists analyze complex datasets they also need to understand the insights collected. Data Visualization will make it easier for them to understand through graphs and charts.\r\n\r\nBest Data Visualization Tools Data Scientists Need To Use\r\n \r\nNowadays, to hire an Android developer or iOS developer depends upon the kind of tools and techniques that they use to an extent. As for businesses around the world, using these tools can help gain business insights and stay ahead in the race. The majority of top iOS and Android mobile app development companies are using these tools to analyze the data sets extracted from mobile apps to help the business grow and maintain a customer base.\r\n\r\nHere are some of the best data visualization tools every Data Scientist must use for the year 2020:\r\n1. Tableau\r\n \r\nIt is an interactive data visualization software. This tool is used for effective data analysis and data visualization in the industry. It has a drag and drop interface and this feature helps it to perform tasks easily and very fast.\r\n\r\nThe software doesn’t force its users to write codes. The software is compatible with a lot of data sources. The tool is a bit expensive but it is the most preferred choice of a top company like Amazon. Qlik view is the biggest competitor of tableau and the tool is extensively used because of its unique drag and drop feature.\r\n\r\nKey features of Tableau:\r\n\r\nTableau is known as the simplest business intelligence tool for data visualization\r\nData scientists do not need to write custom code in this tool\r\nThe tool is also a real-time collaboration along with data mixing\r\n \r\n2. D3\r\n \r\nD3.js is a Javascript library for producing interactive data visualizations in web browsers. It is the most effective platform to work on data visualization. The tool was initially released on Feb 18, 2011, and became official in August.\r\n\r\nIt supports HTML, CSS, and SVG. Developers can present data in the form of creative pictures and graphics. It is a very flexible platform as it allows variations for the creation of different graphs.\r\n\r\nKey features of D3:\r\n\r\nThis data visualization tool offers powerful SVG operation capability\r\nD3 integrates multiple methods as well as tools for the processing of data\r\nData scientists can effortlessly map their data to the SVG attribute\r\n \r\n3. Qlikview\r\n \r\nQlikView is a software similar to a tableau but you need to pay before using it for commercial purposes. It is a business intelligence platform that turns data into useful information.\r\n\r\nThis software helps to improve the data visualization process. The tool is preferred by well-established data scientists to analyze large scale data. Qlik view is used across 100 countries and has a very strong community.\r\n\r\nKey features of QlikView:\r\n\r\nThe tool integrates with a very wide range of data sources such as EC2, Impala, HP Vertica, etc\r\nIt is extremely fast when it comes to data analysis\r\nThis data visualization tool is easily deployable as well as configurable\r\n \r\n\r\n4. Microsoft Power BI\r\n \r\nIt is a set of business analytics tools that can simplify data, prepare and analyze instantly. It is the most preferred tool as it can easily integrate with Microsoft tools and is absolutely free to use and download.\r\n\r\nThe tool is available for both mobile and desktop versions. So if a business uses Microsoft tools it can be a big benefit for them.\r\n\r\nKey features of Microsoft Power BI:\r\n\r\nGenerate interactive data visualizations across multiple data centers\r\nIt offers enterprise data analytics as well as self-service on a single platform\r\nEven non-data scientists can easily create machine learning models\r\n \r\n\r\n5. Datawrapper\r\n \r\nThis tool is a blessing for non-technical users and is the most user-friendly visualization tool. To create visualizations you need to have technical skills such as coding but in this app, you don’t need to have any technical skills.\r\n\r\nThe app can be best used by beginners who want to start their career in data visualization. This app is the most user-friendly app for a data scientist. The tool is widely used in media organizations where there is a high need for presenting everything through stats and graphs. The tool is the most popular choice because it has a simple and easy interface.\r\n\r\nKey features of Datawrapper:\r\n\r\nIt offers the users with an embed code and provides the ability to export charts as well\r\nOption to select multiple map types and charts at once\r\nThe tool requires no advanced knowledge of coding for its installation\r\n \r\n\r\n6. E Charts\r\n \r\nNext, we have in the list of best data visualization tools is E Charts which is an enterprise-level chart data visualization tool from the expert team of Baidu. E Charts can be referred to as a pure Javascript chart library that runs smoothly on various platforms and is also compatible with the majority of browsers.\r\n\r\nKey features of E Charts:\r\n\r\nHas multidimensional data analysis\r\nCharts are available for all sized devices\r\nIt provides a framework for the rapid construction of web-based visualizations.\r\nThese are absolutely free to use\r\n \r\n\r\n7. Plotly\r\n \r\nPlotly enables more complicated and intricate visualizations. It creates a way to its integration with analytics-orientated programming languages consisting of Python, Matlab, and R.\r\n\r\nIt is constructed on top of the open supply d3.Js visualization libraries for JavaScript, but this commercial package (with a potential non-industrial license available) adds layers of user-friendliness and support in addition to inbuilt support for APIs inclusive of Salesforce.\r\n\r\nKey features of Plotly:\r\n\r\nIt offers built-in permissions and integrations with SAML\r\nSuper quick and easy deployment of the data visualization tool\r\nProvides access to users for rapid exploration and prototyping\r\n \r\n\r\n8. Sisense\r\n \r\nA complete analytics solution is provided by Sisense. The visualization abilities offer an uncomplicated drag and drop option that can easily support complicated graphics, charts and interactive visualizations.\r\n\r\nIt allows the accumulation of records in easily accessible repositories where it can be saved instantly on the dashboards.\r\n\r\nDashboards can then be shared throughout groups making sure even the non-technical-minded personnel can discover the solutions they need to their problems.\r\n\r\nKey features of Sisense:\r\n\r\nOffers users with various tools to understand collected data in a visual environment\r\nYou can connect directly to multiple data sources at once\r\nWith this tool, data scientists can tie together various maps and charts\r\n \r\n\r\n9. FusionCharts\r\n \r\nFusionCharts is based on the charting of JavaScript. This visualization tool has secured itself as one of the leaders in the market.\r\n\r\nIt can produce 90 one of a kind chart kinds and integrates with a big variety of systems and frameworks giving a notable deal of flexibility.\r\n\r\nFusionCharts can create any type of visualization from scratch and this is one of its unique features. Customers also have the option to choose from a selection of “live” example templates.\r\n\r\nKey features of FusionCharts:\r\n\r\nIt provides informative tooltips to assist users\r\nThe tool makes sure that users can understand different functionalities\r\nYou can compare the values of different data points with one another\r\n \r\n\r\n10. HighCharts\r\n \r\nLike FusionCharts, this also requires a license for business use, although it may be used freely as a trial, non-business or for non-public use.\r\n\r\nIts internet site claims that it\'s used by seventy-two of the world’s a hundred largest agencies and it is often selected when a quick and flexible solution has to be rolled out, with a minimum need for specialist statistical visualization training before it may be put to work.\r\n\r\nKey features of HighCharts:\r\n\r\nThe tool for data visualization provides its users with good compatibility\r\nHighCharts is one of the most widely used tools for data analyzing\r\nThis tool is convenient to add interactive charts to advanced applications\r\n \r\n\r\nFinal Word\r\n \r\nIn this article, we came across a list of great recorded visualization tools. Before choosing the tool, it is suggested that you spend some time exploring the various potential options.\r\n\r\nGo via the loose trial version, request a demo from the vendor and compare the tool with its nearest competitor tools of equal type. Match the features and pricing plans offered by the vendor against your company and task needs.', 'about-bg.jpg', '2020-07-30 18:10:52'),
(2, 'Data is beautiful', '7 of the best data visualization examples from history to today', 'second-post', 'While data visualization often conjures thoughts of business intelligence with button-down analysts, it’s usually a lot more creative and colorful than you might think. There are many wide-ranging applications from business dashboards to public health visualizations to pop culture trend breakdowns. Great and beautiful data visualization requires graphic design and storytelling skills in addition to great analysis skills.\r\n\r\nIn this article, we’re going to highlight some of the most influential, most interesting, and most revealing visualizations out there. We’ll look at some notable historical examples first and then fast forward and discuss some more contemporary visualizations. Also, be sure to check out our detailed guide to data visualization or check out some of our favorite examples.\r\n\r\n1. Napoleon March Map\r\nIn 1812, Napoleon marched to Moscow in order to conquer the city. It was a disaster: having started with around 470,000 soldiers, he returned with just 10,000. This chart tells the story of that campaign and has become one of the most famous visualizations of all time.\r\n\r\nThe map details the out-and-back journey of Napoleon’s troops. The width of the line represents the total number of soldiers and the color represents the direction (yellow for towards Moscow, black for the return trip). Below the central visualization is also a simple temperature line graph illustrating the rapidly dropping winter cold. It is effective, detailed, and paints a staggering picture of the journey’s devastation.\r\n\r\nBecause of its fame, there is a lot of critical commentary about this chart (this post from Excelcharts.com is a good example). A lot of it is reasonable criticism, but this remains a hugely influential and successful chart, one which excels in telling a story with rich detail at each data point and encourages curiosity.\r\n\r\n2. 1854 Broad Street Cholera Outbreak Map\r\nThe John Snow Cholera map (not the Game of Thrones’ Jon Snow) is essentially an early dot map visualization. It uses small bar graphs on city blocks to mark the number of cholera deaths at each household in a London neighborhood. The concentration and length of these bars show a specific collection of city blocks in an attempt to discover why the trend of deaths is higher than elsewhere. The finding: the households that suffered the most from cholera were all using the same well for drinking water. At the time, this was a complete revelation. The well in question serviced an area with a high concentration of cholera outbreak—and that well had been contaminated by sewage. When applied to the wider trend of London cholera outbreaks, this finding helped trace a clearer line between the sickness and contaminated water wells. The solution to staving off cholera, then, was to build sewage systems and protect wells from contamination.\r\n\r\nThat’s why this is such a hugely successful visualization: it revealed a root cause problem and inspired a solution. Plus, in a time where dot maps and heat maps weren’t yet fully pioneered, this early attempt was incredibly innovative. The solution was only discovered because the analyst pushed the boundaries of visualization technique to create something useful and new.\r\n3. Causes of Mortality in the Crimean War\r\nDuring the Crimean War of the 1850s, soldier mortality rate was high and climbing. But not just because of the battles. Nurse, analyst, and data rockstar Florence Nightingale used this beautiful data visualization to reveal that the majority of deaths were actually caused by poor hospital practices. The shaded areas of the spiral charts show total deaths, and the darker shaded areas represent deaths due to combat. It is quick and easy to tell that something else was going on here, and on a large scale. Nightingale’s medical expertise and hospital visits gave her insight to identify the bad medical conditions as the surprising and preventable cause of so much death.\r\n\r\nThe research was part of a Royal Commission looking into the causes of mortality of soldiers in the Crimean War. Nightingale worked with William Farr, a Victorian pioneer in statistics who did not support the idea of including visualizations, but Nightingale stood firm and advocated for this now-famed visualization.\r\n4. New Chart of History\r\nProlific and visual way to imagine the massive and complex timeline of recorded human civilization.\r\n\r\nJoseph Priestly is well known for two timeline charts. The first is the Chart of Biography, which provided a 700-year timeline of famous men, leaders, and philosophers, and drew focus to which men were active in history at the same time. The Chart of Biography, as simple as it is, remains one of history’s most important visualizations.\r\n\r\nHis second timeline evolves the techniques in the Chart of Biography to create the New Chart of History. Similar to the first chart, this is a timeline that draws focus to the simultaneous existence and influence of major empires and cultures through history. Priestly innovates on his technique by introducing color, size, and a creative y-axis of location. The result is a fascinating visual story of history that speaks volumes. While it is a very visually busy chart, it is also endlessly creative and was an original and huge innovation at the time.\r\n5. Interactive Government Budget\r\nAll governments, and particularly the USA, have notoriously obscure and tough to understand government budgets. This treemap, created by The White House during Barack Obama’s presidency, visually broke down the United State’s 2016 the budget to put government programs in context. It’s unclear if this was the first ever interactive budget publicly published by the US government, but it remains archived and illuminating—even if it is a fairly basic treemap chart.\r\n\r\nWhat makes this particular visualization so important is the delivery method. This isn’t the most innovative treemap, nor the most innovative interactive visualization, and it wasn’t the first widely-known visualization of a government budget (the New York Times had an incredible 2013 budget viz and candidate Ross Perot was well-known for his use of charts). The key thing here is the fact that a major world power adopted interactive data visualization as way to communicate with taxpayers about where their tax dollars would go. This complicated and obscure topic was made accessible with a simple and clear visualization.\r\n\r\n6. Film Dialogue (broken down by gender)\r\nWhile Polygraph (aka The Pudding) is perhaps better known for a certain rap lyrics breakdown visualization, here Hanah Anderson and Matt Daniels visualize gender disparity in pop culture by breaking down the scripts for 2000 of the biggest movies in cinema history. Each movie tallies up spoken lines of dialogue for male and female characters and the findings are severe.\r\n\r\nIt’s easy to notice the lack of female-led action movies, but it’s a whole other matter to visualize the absolute stark imbalance in gender representation for every genre. This project hosts four main visualizations: a breakdown of Disney movies specifically, an overview of 2000 scripts, a simple gradient bar that allows the user to search for movies and explore a few key filters, and a brief look into the age biases shown towards male and female roles.\r\n\r\nIn addition to the impressive work of analyzing 2000 scripts and presenting the striking findings, this project is notable for its frank transparency: the data and methodology are public and detailed and are presented within the project itself. This kind of transparency is an incredibly welcome but slow-growing trend.\r\n7. Selfiecity\r\nDon’t pass over this one just because it analyzes selfies instead of cholera prevention. Selfiecity presents a wide view of selfie data in the context of a transnational phenomenon. 120,000 selfies from around the world are analyzed to study how people take selfies. What’s incredible here is just how comprehensive the study is and how seriously it slices and dices every aspect of selfies. We can find trends in everything from head tilt or pose trends by city to smile frequency by age group and gender.\r\n\r\nIt probably won’t surprise you to learn that the selfie-takers tended to be young. But it may surprise you to learn that selfies aren’t as prevalent as is usually assumed, that women in Sao Paulo selfies favor an extreme head tilt compared to the rest of the world, and that Bangkok is all smiles. With social media influence becoming more ingrained in our lives, this is a fascinating look at a prolific global phenomenon. As a bonus, there’s an interactive element that allows users to apply filters in a unique way to further explore the world of selfies.', 'about-bg.jpg', '2020-07-30 18:19:33'),
(3, 'Top 10 Python Libraries You Must Know In 2020', 'Top 10 Python Libraries ', 'third-post', 'What Is TensorFlow?\r\nIf you are currently working on a machine learning project in Python, then you may have heard about this popular open source library known as TensorFlow.\r\n\r\nThis library was developed by Google in collaboration with Brain Team. TensorFlow is used in almost every Google application for machine learning.\r\n\r\nTensorFlow works like a computational library for writing new algorithms that involve a large number of tensor operations, since neural networks can be easily expressed as computational graphs they can be implemented using TensorFlow as a series of operations on Tensors. Plus, tensors are N-dimensional matrices which represent your data.\r\n\r\nFeatures of TensorFlow\r\nTensorFlow is optimized for speed, it makes use of techniques like XLA for quick linear algebra operations.\r\n\r\n1. Responsive Construct\r\n\r\nWith TensorFlow, we can easily visualize each and every part of the graph which is not an option while using Numpy or SciKit.\r\n\r\n2. Flexible\r\n\r\nOne of the very important Tensorflow Features is that it is flexible in its operability, meaning it has modularity and the parts of it which you want to make standalone, it offers you that option.\r\n\r\n3. Easily Trainable\r\n\r\nIt is easily trainable on CPU as well as GPU for distributed computing.\r\n\r\n4. Parallel Neural Network Training\r\n\r\nTensorFlow offers pipelining in the sense that you can train multiple neural networksand multiple GPUs which makes the models very efficient on large-scale systems.\r\n\r\n5. Large Community\r\n\r\nNeedless to say, if it has been developed by Google, there already is a large team of software engineers who work on stability improvements continuously.\r\n\r\n6. Open Source\r\nThe best thing about this machine learning library is that it is open source so anyone can use it as long as they have internet connectivity.\r\n\r\nWhere Is TensorFlow Used?\r\nYou are using TensorFlow daily but indirectly with applications like Google Voice Search or Google Photos. These applications are developed using this library.\r\nAll the libraries created in TensorFlow are written in C and C++. However, it has a complicated front-end for Python. Your Python code will get compiled and then executed on TensorFlow distributed execution engine built using C and C++.\r\nThe number of applications of TensorFlow is literally unlimited and that is the beauty of TensorFlow.\r\n\r\n2.Scikit-Learn\r\nWhat Is Scikit-learn?\r\nIt is a Python library is associated with NumPy and SciPy. It is considered as one of the best libraries for working with complex data.\r\n\r\nThere are a lot of changes being made in this library. One modification is the cross-validation feature, providing the ability to use more than one metric. Lots of training methods like logistics regression and nearest neighbors have received some little improvements.\r\n\r\nFeatures Of Scikit-Learn\r\n1. Cross-validation: There are various methods to check the accuracy of supervised models on unseen data.\r\n\r\n2. Unsupervised learning algorithms: Again there is a large spread of algorithms in the offering – starting from clustering, factor analysis, principal component analysis to unsupervised neural networks.\r\n\r\n3. Feature extraction: Useful for extracting features from images and text (e.g. Bag of words)\r\n\r\n\r\nWhere Is Scikit-Learn Used?\r\nIt contains a numerous number of algorithms for implementing standard machine learning and data mining tasks like reducing dimensionality, classification, regression, clustering, and model selection.\r\n\r\n3.Numpy\r\nWhat Is Numpy?\r\nNumpy is considered as one of the most popular machine learning library in Python.\r\n\r\nTensorFlow and other libraries uses Numpy internally for performing multiple operations on Tensors. Array interface is the best and the most important feature of Numpy.\r\n\r\nFeatures Of Numpy\r\nInteractive: Numpy is very interactive and easy to use.\r\nMathematics: Makes complex mathematical implementations very simple.\r\nIntuitive: Makes coding real easy and grasping the concepts is easy.\r\nLot of Interaction: Widely used, hence a lot of open source contribution.\r\nWhere Is Numpy Used?\r\nThis interface can be utilized for expressing images, sound waves, and other binary raw streams as an array of real numbers in N-dimensional.\r\n\r\nFor implementing this library for machine learning having knowledge of Numpy is important for full stack developers.\r\n\r\n4.Keras\r\nWhat Is Keras?\r\nKeras is considered as one of the coolest machine learning libraries in Python. It provides an easier mechanism to express neural networks. Keras also provides some of the best utilities for compiling models, processing data-sets, visualization of graphs, and much more.\r\n\r\nIn the backend, Keras uses either Theano or TensorFlow internally. Some of the most popular neural networks like CNTK can also be used. Keras is comparatively slow when we compare it with other machine learning libraries. Because it creates a computational graph by using back-end infrastructure and then makes use of it to perform operations. All the models in Keras are portable.\r\n\r\nFeatures Of Keras\r\nIt runs smoothly on both CPU and GPU.\r\nKeras supports almost all the models of a neural network – fully connected, convolutional, pooling, recurrent, embedding, etc. Furthermore, these models can be combined to build more complex models.\r\nKeras, being modular in nature,  is incredibly expressive, flexible, and apt for innovative research.\r\nKeras is a completely Python-based framework, which makes it easy to debug and explore.\r\nWhere Is Keras Used?\r\nYou are already constantly interacting with features built with Keras — it is in use at Netflix, Uber, Yelp, Instacart, Zocdoc, Square, and many others. It is especially popular among startups that place deep learning at the core of their products.\r\n\r\nKeras contains numerous implementations of commonly used neural network building blocks such as layers, objectives, activation functions, optimizers and a host of tools to make working with image and text data easier. \r\n\r\nPlus, it provides many pre-processed data-sets and pre-trained models like MNIST, VGG, Inception, SqueezeNet, ResNet etc.\r\n\r\nKeras is also a favorite among deep learning researchers, coming in at #2. Keras has also been adopted by researchers at large scientific organizations, in partic,ular CERN and NASA.\r\n\r\n5.PyTorch\r\nWhat Is PyTorch?\r\nPyTorch is the largest machine learning library that allow developers to perform tensor computations wan ith acceleration of GPU, creates dynamic computational graphs, and calculate gradients automatically. Other than this, PyTorch offers rich APIs for solving application issues related to neural networks.\r\n\r\nThis machine learning library is based on Torch, which is an open source machine library implemented in C with a wrapper in Lua.\r\n\r\nThis machine library in Python was introduced in 2017, and since its inception, the library is gaining popularity and attracting an increasing number of machine learning developers.\r\n\r\nFeatures Of PyTorch\r\nHybrid Front-End\r\n\r\nA new hybrid front-end provides ease-of-use and flexibility in eager mode, while seamlessly transitioning to graph mode for speed, optimization, and functionality in C++ runtime environments.\r\n\r\nDistributed Training\r\n\r\nOptimize performance in both research and production by taking advantage of native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++.\r\n\r\nPython First\r\n\r\nPyTorch is not a Python binding into a monolithic C++ framework. It’s built to be deeply integrated into Python so it can be used with popular libraries and packages such as Cython and Numba.\r\n\r\nLibraries And Tools\r\n\r\nAn active community of researchers and developers have built a rich ecosystem of tools and libraries for extending PyTorch and supporting development in areas from computer vision to reinforcement learning.\r\n\r\nWhere Is PyTorch Used?\r\nPyTorch is primarily used for applications such as natural language processing.\r\n\r\nIt is primarily developed by Facebook’s artificial-intelligence research group and Uber’s “Pyro” software for probabilistic programming is built on it.\r\n\r\nPyTorch is outperforming TensorFlow in multiple ways and it is gaining a lot of attention in the recent days.\r\n\r\n6.LightGBM\r\nWhat Is LightGBM?\r\nGradient Boosting is one of the best and most popular machine learning library, which helps developers in building new algorithms by using redefined elementary models and namely decision trees. Therefore, there are special libraries which are designed for fast and efficient implementation of this method.\r\n\r\nThese libraries are LightGBM, XGBoost, and CatBoost. All these libraries are competitors that helps in solving a common problem and can be utilized in almost the similar manner.\r\n\r\nFeatures of LightGBM\r\nVery fast computation ensures high production efficiency.\r\n\r\nIntuitive, hence makes it user friendly.\r\n\r\nFaster training than many other deep learning libraries.\r\n\r\nWill not produce errors when you consider NaN values and other canonical values.\r\n\r\nWhere Is LightGBM Used?\r\nThese library provides provide highly scalable, optimized, and fast implementations of gradient boosting, which makes it popular among machine learning developers. Because most of the machine learning full stack developers won machine learning competitions by using these algorithms.\r\n\r\n7.Eli5\r\nWhat Is Eli5?\r\nMost often the results of machine learning model predictions are not accurate, and Eli5 machine learning library built in Python helps in overcoming this challenge. It is a combination of visualization and debug all the machine learning models and track all working steps of an algorithm.\r\n\r\nFeatures of Eli5\r\nMoreover, Eli5 supports wother libraries XGBoost, lightning, scikit-learn, and sklearn-crfsuite libraries. All the above-mentioned libraries can be used to perform different tasks using each one of them.\r\n\r\nWhere Is Eli5 Used?\r\nMathematical applications which requires a lot of computation in a short time.\r\n\r\nEli5 plays a vital role where there are dependencies with other Python packages.\r\n\r\nLegacy applications and implementing newer methodologies in various fields.\r\n\r\n8.Scipy\r\nWhat Is SciPy?\r\nSciPy is a machine learning library for application developers and engineers. However, you still need to know the difference between SciPy library and SciPy stack. SciPy library contains modules for optimization, linear algebra, integration, and statistics.\r\n\r\nFeatures Of SciPy\r\nThe main feature of SciPy library is that it is developed using NumPy, and its array makes the most use of NumPy.\r\n\r\nIn addition, SciPy provides all the efficient numerical routines like optimization, numerical integration, and many others using its specific submodules.\r\n\r\nAll the functions in all submodules of SciPy are well documented.\r\n\r\nWhere Is SciPy Used?\r\nSciPy is a library that uses NumPy for the purpose of solving mathematical functions. SciPy uses NumPy arrays as the basic data structure, and comes with modules for various commonly used tasks in scientific programming.\r\n\r\nTasks including linear algebra, integration (calculus), ordinary differential equation solving and signal processing are handled easily by SciPy.\r\n\r\n9.Theano\r\nWhat Is Theano?\r\nTheano is a computational framework machine learning library in Python for computing multidimensional arrays. Theano works similar to TensorFlow, but it not as efficient as TensorFlow. Because of its inability to fit into production environments.\r\n\r\nMoreover, Theano can also be used on a distributed or parallel environments just similar to TensorFlow.\r\n\r\nFeatures Of Theano\r\nTight integration with NumPy – Ability to use completely NumPy arrays in Theano-compiled functions.\r\nTransparent use of a GPU – Perform data-intensive computations much faster than on a CPU.\r\nEfficient symbolic differentiation – Theano does your derivatives for functions with one or many inputs.\r\nSpeed and stability optimizations – Get the right answer for log(1+x) even when x is very tiny. This is just one of the examples to show the stability of Theano.\r\nDynamic C code generation – Evaluate expressions faster than ever before, thereby, increasing efficiency by a lot.\r\nExtensive unit-testing and self-verification – Detect and diagnose multiple types of errors and ambiguities in the model.\r\nWhere Is Theano Used?\r\nThe actual syntax of Theano expressions is symbolic, which can be off putting to beginners used to normal software development. Specifically, expression are defined in the abstract sense, compiled and later actually used to make calculations.\r\n\r\nIt was specifically designed to handle the types of computation required for large neural network algorithms used in Deep Learning. It was one of the first libraries of its kind (development started in 2007) and is considered an industry standard for Deep Learning research and development.\r\n\r\nTheano is being used in multiple neural network projects today and the popularity of Theano is only growing with time.\r\n\r\n10.Pandas\r\nWhat Is Pandas?\r\nPandas is a machine learning library in Python that provides data structures of high-level and a wide variety of tools for analysis. One of the great feature of this library is the ability to translate complex operations with data using one or two commands. Pandas have so many inbuilt methods for grouping, combining data, and filtering, as well as time-series functionality.\r\n\r\nAll these are followed by outstanding speed indicators.\r\n\r\nFeatures Of Pandas\r\nPandas make sure that the entire process of manipulating data will be easier. Support for operations such as Re-indexing, Iteration, Sorting, Aggregations, Concatenations and Visualizations are among the feature highlights of Pandas.\r\n\r\nWhere Is Pandas Used?\r\nCurrently, there are fewer releases of pandas library which includes hundred of new features, bug fixes, enhancements, and changes in API. The improvements in pandas regards its ability to group and sort data, select best suited output for the apply method, and provides support for performing custom types operations.\r\n\r\nData Analysis among everything else takes the highlight when it comes to usage of Pandas. But, Pandas when used with other libraries and tools ensure high functionality and good amount of flexibility.\r\n\r\n\r\n\r\n\r\n', 'about-bg.jpg', '2020-07-30 20:36:47'),
(4, 'What is The Difference Between Data Science and Machine Learning?', 'Difference Between Data Science and Machine Learning', 'fourth-post', 'Whether it is Apple’s Siri or Amazon’s Echo, Artificial Intelligence and machine learning is slowly taking over our lives as modern-day assistants. If you look at the larger picture, AI is also becoming a part of every growing business, with more people getting acquainted with technical terms like big data, data sciences, and machine learning, and using them to solve complex analytical problems. \r\n\r\nWith ample data to process, companies use data science techniques in discovering, understanding and analyzing the complex, raw data resting in their databases. Machine learning is a part of data science, uses algorithms and statistics to understand the extracted data.  While both data science and machine learning differ in functionality and purpose, you may often confuse the two to be aspects of the same technology; this post aims to break down the difference between data science and machine learning and their applicability.  \r\n\r\nUnderstanding Data Science\r\nPicture a scenario where you are asked to use technology and solve an imminent business problem. Where would you start? You’d probably start by identifying the problem first so that you get a clearer perspective of how to solve it. This is where data science fits the bill! \r\n\r\nData science is an extensive study of data. It is used for analyzing and processing data through algorithm developments, data inference to simplify complex analytical issues and extract information. Have you noticed how after you’ve looked at a particular product on Amazon, multiple ads of the same product pop up on your screen when you’re catching a show on YouTube or Netflix? That’s data science doing its job for you! In simpler terms, data science uses data, both in streaming and raw format to generate business value. \r\n\r\nSkills required in the field of data science\r\nTo explore career prospects in data science, here are a couple of required skills:\r\n\r\nExpertise in mathematics\r\n\r\nThere are multiple facets of data, including correlations, textures, and dimensions that need to be expressed mathematically or statistically. For building a data product and lending data insights, expertise in mathematics is given must.  \r\n\r\nHacking and technology expertise\r\n\r\nBreathe! By hacking, we don’t mean breaking into someone’s computer. It essentially means applying your ingenuity and creativity to manipulate technical knowledge and find solutions to build ideas and products for businesses. \r\n\r\nStrong strategy or business acumen\r\n\r\nAmong the crucial skills for any data scientist is to be proficient in tactical business. It is necessary to be competent in tackling data to cogently offer a solution or offer a more cohesive narrative of a complex issue and a solution for the said problem. \r\n\r\nUnderstanding Machine learning \r\nMachine learning is a branch of Artificial Intelligence that enables a computer to learn automatically from experience with any kind of human intervention. The whole concept of machine learning revolves around determining answers to obstacles without human interference, which begins with understanding data from examples or direct experiences, analyse data patterns and make better decisions based on the deductions. \r\n\r\nIt is best used for problem-solving when there are extensive data and variables without using existing algorithms. For example, Google tends to optimize search results and pops up advertisements of products that are either similar to your taste or websites that you had previously visited. It studies the behavior of a user and shows results accordingly.\r\n\r\nSkills required for machine learning\r\nA professional interested in the field of machine learning needs to be skilled in the following:\r\n\r\nExpertise in probability and statistics\r\n\r\nA deep understanding of algorithms, expertise probability of drawing inferences from data and make predictive models, using statistics to understand p-values and solve confusion matrices are crucial in the field of machine learning. \r\n\r\nKnowledge of programming languages\r\n\r\nMachine learning without programming languages is as good as an empty glass! Extensive knowledge of programming languages like C++, Python, Java, R and more is crucial. \r\n\r\nData modelling and evaluation skills\r\n\r\nAny machine learning process is incomplete without the evaluation of a given data model. To be skilled in machine learning, a professional needs to possess an understanding of how data modelling works, what accuracy measures would be appropriate for a given error and also have a working evaluation strategy. \r\n\r\nAdditional skills\r\n\r\nApart from these skills, being in sync with the latest development tools, algorithms, and theory can come handy too. Reading papers on Google Big Table, Google File System, Google Map-Reduce can be useful. \r\n\r\nConclusion\r\nMachine learning is a component of data science; where data science as the larger picture comprises of big data, data learning, statistics and much more. Machine learning involves the use of programming and computational algorithms to arrive at a conclusion, whereas data science uses numbers and statistics to bring a result. \r\n\r\nFor companies that are more data-driven, switching to data science is a secret mantra for enhancing business and for targeting better returns on investments. Machine learning, on the other hand, in today’s date, is essential since it can solve intricate and complex computational problems by breaking them down into bits.', 'about-bg.jpg', '2020-08-02 13:21:27'),
(5, 'How Is Data Science Changing Web Design?', 'Data Science Changing Web Design?', 'fifth-post', 'Data science isn’t just changing web design in a minor way: it’s changing every aspect of it from the start of the design process to the end (and even beyond through the update process). Whenever you have the resources and expertise to deploy it, it’s worthwhile, because having cut-and-dry insight into performance is invaluable.\r\n\r\nWhen the internet first began to pick up some steam, it had some of the hallmarks of the Wild West: the rules hadn’t been clearly defined, for instance, and direct competition was fairly minimal because there was so much space to be filled. As a result, there was a lot of freewheeling experimentation going by gut feeling. Just try something, see how it goes.\r\n\r\nAs the years have gone by, the online world has gone from an attention-grabbing novelty to a fundamental part of daily life. Along the way, its standards have changed immensely, and there are two reasons for this: technological progress in general, and frequent efforts from online brands to exceed previous levels of service and performance to outperform their rivals.\r\n\r\nGiven how competitive the web is now, going by gut feeling won’t get brands very far, so they are essentially required to invest in data science: using the smartest methods available to them to analyze relevant data and reach informed conclusions about what they need to do. In this post, we’re going to consider how data science is changing web design. Let’s begin:\r\n\r\nIT’S INFORMING EVERY STEP OF THE PROCESS\r\nThe significance of data science is such that it doesn’t simply factor into one element of the web design process (weighing in at the end, perhaps) — instead, it has a key role to play at every point and in every department. The reasoning for this should be fairly obvious. Given that every step in a digital process inevitably produces trackable results (without trackable results, you can’t know how well something is working), the opportunity for data science is always there.\r\n\r\nIt may not be used at all opportunities when small businesses run web design projects, but that’s simply because they don’t have the resources to invest so broadly. Look at big companies to see where things are going. Case in point: eCommerce giant Shopify (known for its sell everything everywhere message) has a full Data Science & Engineering team, but it doesn’t serve exclusively as a separate unit.\r\n\r\nInstead, as department VP Solmaz Shahalizadeh said in this interview with Matt Turck, “Data scientists are embedded in different business units or product areas. They work closely with the product managers, UX researchers and development teams.” By saturating the design process with data scientists, the department ensures that feedback has maximum utility.\r\n\r\nIT’S OFFERING IMPROVED USER EXPERIENCES\r\nPutting so much time and effort into data science wouldn’t be worthwhile if it didn’t ultimately get impressive results, so we need to think about the net product of the investment: improved user experiences. Excellent web design needs to allow users to find what they need with minimal effort and optimal convenience. In addition to being easy to operate, it should find other ways to impress the user: the bigger a positive impression it can make, the better.\r\n\r\nNow think about all the customized and personalized experiences you can now get in the online world. You can visit a familiar retailer and benefit from dynamic recommendations that factor in your previous purchases, deals that suit your interests, and even design elements that reflect your preferences (not common, but not unheard of).\r\n\r\nAll of these things are made possible by the data science field: without machine learning to automate them, dynamic recommendations would need to be done manually. You can even think about chatbot-enhanced customer support. Being able to check up on the status of an order by issuing a request through a chatbot window feels like a trivial thing, but it was made possible by advances in natural language processing that couldn’t have been achieved without deploying machine learning to process and parse vast quantities of data.\r\n\r\nIT’S STARTING TO AUTOMATE UPDATE PATHS\r\nWeb designs aren’t supposed to be static. Once you deploy them, it won’t be very long before they’re outdated relative to newer designs, so you need to make a commitment to keeping them updated. But what if you didn’t need to slowly work on manual updates and carefully roll them out as appropriate? What if you could automate much of the update process?\r\n\r\nWell, with the right process in place (powered by data science, of course), you could. You could set up a system with various flexible elements and have it reshuffle those elements in response to analytics from testing. Tweak one element and see how the results change: if they get worse, revert the tweak, and if they get better, leave it as it is and tweak something else.\r\n\r\nIn the long run, self-optimizing systems are going to become extremely common. Human intelligence can then be put towards more interesting things, such as new projects or updates that go beyond basic comparison testing. The key will be finding a balance between the things at which computers excel (repetitive processes at massive scale) and the things at which human brains excel (complex issues requiring creative thinking).\r\n\r\n', 'post-bg.jpg', '2020-08-02 13:24:45');
INSERT INTO `posts` (`sno`, `title`, `tagline`, `slug`, `content`, `img_file`, `date`) VALUES
(6, 'Data Science & Cyber Security: 5 Reasons Why Digital Economy Cannot Do Without Data science and cyber security', 'Why Digital Economy Cannot Do Without Data science and cyber security', 'sixth-post', 'Consider Data Science & Cyber Security as two powerful engines empowering the digital economy of today and tomorrow. The premise of digital economics relies on the inter-relationship between data, business value and managerial decision making. With increased adoption of digitisation and digital transformation, we will consume more data than ever before. Rightly, it’s considered the new oil of modern times. As Alphabet’s Eric Schmidt states, in every 48 hours, we generate more data than humanity produced since the dawn of civilization until 15 years ago. The big question is, how are we going to make a meaning of all that? Data Science is not just a buzzword. Today, no company can prosper without the insight of data. That’s why, if you compare Fortune 500 companies of this year with the 10-year-old list, you will realize only the data driven companies have performed consistently and have become the new winners of the digital economy. \r\n\r\nAnd, as we move the world of brick and mortar into the digital world along the new and emerging models of businesses, the risk of security also crops up exponentially. Yes, information technology has bridged the gap, between nations, companies, buyers and sellers and markets, but it also means we are increasingly leaving our digital footprints along the way. And some entities are storing that data. So, no matter how secured an organisation’s or country’s data may look unbeatable, it may actually fall apart one fine morning and expose personal, financial and sensitive data out in the open. \r\n\r\nFive reasons why it’s important for the digital economy to leverage data science and cyber security for its own good. \r\n\r\nData Science \r\n\r\n1 )Data Storage and Retrieval: Story of data science originates from storing of data. We have stored them in our heads, earthen slates, paper and then on the computer. Today’s onslaught of big data anyway has to be collected and extracted. With the abundance of IoT devices endlessly generating and transmitting data, businesses have to store and retrieve a high-volume, high-velocity, and high-variety unstructured data.\r\n2 )Data Cleansing: You have a lot of data at your disposal but a good percentage of them are useless, outdated, incorrect or difficult to format. The challenge here is to create a nice, easy to use formatting and conforming to internal quality rules. Many believe data sparseness and formatting inconsistencies are the biggest challenges. When more data pours in, the spreadsheet turns into a database and turns into a data warehouse. Without proper data science interventions, companies can’t ensure clean data sets.\r\n3 )Data Analysis: Data analysis is important to understand problems faced by an organisation, a lot of time there may not be any evident problem but by predicting customer trends and behaviours, analysing, interpreting and delivering data meaningfully, businesses can enhance productivity and drive effective decision-making.\r\n4 ) Modelling, statistics: Application of statistical analysis to a dataset holds immense value to any industry be it manufacturing, retail or fintech. Instead of sifting through raw data interprets relationships between variables, predicts future data sets and helps you see patterns. With the help of machine learning and artificial intelligence companies are leveraging statistical models to build representation of data.\r\n5 )Engineering, prototyping: Clean data and a good model is just the beginning. It means developing some sort of data tools or products so that cross-team collaboration can take place and non-data scientists, internal employees like business analysts etc. can use them internally for visualisation, dashboard or applications  \r\nCyber Security\r\n\r\nProtection against malware, ransomware, phishing and social engineering: The cyber security landscape is constantly evolving. Attackers often use a combination of ransomware or social engineering for example – to maximise the impact. No matter how much we heighten our technology around security and make our human folks aware, human elements will remain vulnerable to innovative attacks. Modern digital businesses need to focus on strong firewalls, VPNs and advanced malware, ransomware and phishing protection along with supporting email and endpoint security.\r\n \r\n1 )Protection for data and networks: In its simple form it is a set of rules and configurations designed to protect the integrity, confidentiality and accessibility of computer networks and data using both software and hardware technologies. Due to digitisation, our world has changed. All organisations, regardless of size and scale, connected customers and employees digitally are exposed to the ever growing landscape of cyber threats and must protect its network.\r\n\r\n2 )Prevention of unauthorized users: A security breach or data breach in the form of unauthorised access happens when an attacker successfully gains unauthorised access to an enterprise system namely, data, networks, endpoints, applications or devices. This happens through three stages, the attacker successfully researches the vulnerabilities, evades network defence and then exfiltrates with data. Businesses can protect such attacks through strong password policy, Two Factor Authentication (2FA) and Multifactor Authentication, Physical Security Practices, Monitoring User Activity and Endpoint Security.\r\n\r\n3 )Improves recovery time after a breach: Experts estimate that ransomware attacks are up over 600 percent, says a Microsoft report. Planning for data breaches and having a strategic approach through disaster recovery should be a baseline activity. A good and current offline backup is the first step. Offline backups are out of reach of ransomware and cyber thieves. Having documented centralised logs right before the incident helps forensic to get to the root cause. \r\n\r\n4 )Improved confidence in the product for both developers and customers: Over the years, organisations have become more mature, aware and have placed systems in place as far as business risk assessment is concerned. Despite issues, when leadership expresses confidence in their abilities to protect their organisations from cyber-attacks, it goes well with employees, developers and customers as well.', 'post-bg.jpg', '2020-08-02 13:33:15'),
(7, '5 ML/AI Projects To Make Your Portfolio Stand Out', 'Projects To Make Your Portfolio Stand Out', 'Seventh-post', '1. Sentiment analysis for depression based on social media post :\r\nThis topic is so sensitive to be considered nowadays and in urgent need to do something about it. There are more than 264 million individuals worldwide who are suffering from depression. Depression is the main cause of disability worldwide and is a significant supporter of the overall global burden of disease and nearly 800,000 individuals consistently bite the dust because of suicide every year. Suicide is the second driving reason for death in 15–29-year-olds. Treatment for depression is often delayed, imprecise, and/or missed entirely.\r\nInternet-based life gives the main edge chance to change early melancholy mediation services, especially in youthful grown-ups. Consistently, roughly 6,000 Tweets are tweeted on Twitter, which relates to more than 350,000 tweets sent for each moment, 500 million tweets for every day, and around 200 billion tweets for each year.\r\nAs indicated by the Pew Research Center, 72% of the public uses some sort of internet-based life. Datasets released from social networks are important to numerous fields, for example, human science and brain research. But the supports from a specialized point of view are a long way from enough, and explicit methodologies are desperately out of luck.\r\nBy analyzing linguistic markers in social media posts, it’s possible to create a deep learning model that can give an individual insight into his or her mental health far earlier than traditional approaches.\r\n\r\n2. Sports match video to text summarization using neural network\r\nSo this project idea is basically based on getting precise summary out of Sports match videos. There are sports websites that tell about highlights of the match. Various models have been proposed for the task of extractive text summarization but neural networks do the best job. As a rule, Summarization alludes to introducing information in a brief structure, concentrating on parts that convey facts and information, while safeguarding the importance.\r\nAutomatically creating an outline of a game video gives rise to the challenge of distinguishing fascinating minutes, or highlights, of a game.\r\nSo, one can achieve that using some deep learning techniques like 3D-CNN (three-dimensional convolutional networks), RNN(Recurrent neural network), LSTM (Long short term memory networks) and also through Machine learning algorithms by dividing the video into different sections and then applying SVM(Support vector machines), NN(Neural Networks), k-means algorithm.\r\n\r\n3. Handwritten equation solver using CNN\r\nAmong all the issues, handwritten mathematical expression recognition is one of the confounding issues in the region of computer vision research. You can train Handwritten equation solver by handwritten digits and mathematical symbols using Convolutional Neural Network (CNN) with some image processing techniques. Developing such a system requires training our machines with data, making it proficient to learn and make the required prediction.\r\n\r\n4. Business meeting summary generation using NLP\r\nEver got stuck in a situation, where everyone wants to see a summary not full reports. Well, I face it during my school and college days where we spend a lot of time preparing a whole report but the teacher only has time to read the summary.\r\nSummarization has risen as an inexorably helpful way to tackle the issue of data over-burden. Extracting information from conversations can be of very good commercial and educational value. This can be done by feature capture of the statistical, linguistic, and sentimental aspects with the dialogue structure of the conversation.\r\nManually changing the report to a summed up form is too time taking, isn’t that so? But one can rely on Natural Language Processing (NLP) techniques to achieve that.\r\nText summarization using deep learning can understand the context of the entire text. Isn’t it a dream come true for all of us who need to come up with a quick summary of a document !!\r\n\r\n5. Facial recognition to detect mood and suggest songs accordingly\r\nThe human face is an important part of an individual’s body and it particularly plays a significant role in knowing a person’s state of mind. This eliminates the dreary and tedious task of manually isolating or grouping songs into various records and helps in generating an appropriate playlist based on an individual’s emotional features.\r\nPeople tend to listen to music based on their mood and interests. One can create an application to suggest songs for users based on their mood by capturing facial expressions.\r\nComputer vision is an interdisciplinary field that helps convey a high-level understanding of digital images or videos to computers. computer vision components can be used to determine the user’s emotion through facial expressions.\r\n', 'post-bg.jpg', '2020-08-02 15:27:39');

--
-- Indexes for dumped tables
--

--
-- Indexes for table `contacts`
--
ALTER TABLE `contacts`
  ADD PRIMARY KEY (`sno`);

--
-- Indexes for table `posts`
--
ALTER TABLE `posts`
  ADD PRIMARY KEY (`sno`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `contacts`
--
ALTER TABLE `contacts`
  MODIFY `sno` int(50) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=8;

--
-- AUTO_INCREMENT for table `posts`
--
ALTER TABLE `posts`
  MODIFY `sno` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=11;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
